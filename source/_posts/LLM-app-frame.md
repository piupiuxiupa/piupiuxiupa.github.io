---
title: LLM-app-frame
date: 2025-11-17 11:05:59
tags: LLM
category: Notes
except: å¤§æ¨¡å‹åº”ç”¨å¼€å‘æ¡†æ¶
---

## Python å¸¸ç”¨æ¡†æ¶

- Langchain
- LlamaIndex

## åŸºäºRAGæ¶æ„çš„å¼€å‘

- å¤§æ¨¡å‹çŸ¥è¯†å†»ç»“
- å¤§æ¨¡å‹å¹»è§‰

## åŸºäºAgentæ¶æ„çš„å¼€å‘  

## æ¨¡å‹è°ƒç”¨çš„åˆ†ç±»

1. æŒ‰ç…§æ¨¡å‹åŠŸèƒ½ä¸åŒï¼š

   - éå¯¹è¯æ¨¡å‹
   - å¯¹è¯æ¨¡å‹
   - åµŒå…¥æ¨¡å‹

2. æŒ‰ç…§æ¨¡å‹è°ƒç”¨æ—¶å‚æ•°ä¹¦å†™ä½ç½®

   - ç¡¬ç¼–ç æ–¹å¼ï¼Œå°†å‚æ•°å†™åœ¨ä»£ç ä¸­
   - ä½¿ç”¨ç¯å¢ƒå˜é‡çš„æ–¹å¼
   - ä½¿ç”¨é…ç½®æ–‡ä»¶çš„æ–¹å¼

3. å…·ä½“APIçš„è°ƒç”¨

   - ä½¿ç”¨LangChainæä¾›çš„API
   - ä½¿ç”¨OpenAI å®˜æ–¹çš„API
   - ä½¿ç”¨å…¶ä»–å¹³å°æä¾›çš„API

```python
# éå¯¹è¯æ¨¡å‹
from langchain_openai import OpenAI
# å¯¹è¯æ¨¡å‹
from langchain_openai import ChatOpenAI
```

## å‚æ•°è®¾ç½®

- model_name
  - æ¨¡å‹åç§°ï¼Œå¦‚"gpt-3.5-turbo"ã€"gpt-4"ç­‰
- base_url
  - æ¨¡å‹APIçš„åŸºç¡€URLï¼Œå¦‚"https://api.openai.com/v1"
- api_key
  - æ¨¡å‹APIçš„å¯†é’¥ï¼Œç”¨äºèº«ä»½éªŒè¯

- temperature
  - æ¸©åº¦ï¼Œæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå–å€¼èŒƒå›´ä¸º0ï½1
    - å€¼è¶Šä½ï¼Œè¾“å‡ºè¶Šç¡®å®šã€ä¿å®ˆï¼Œé€‚åˆäº‹å®å›ç­”
    - å€¼è¶Šé«˜ï¼Œè¾“å‡ºè¶Šå¤šæ ·ã€æœ‰åˆ›æ„ï¼Œé€‚åˆåˆ›æ„å†™ä½œ
  - ç²¾ç¡®æ¨¡å¼
    - 0.5æˆ–æ›´ä½ï¼Œç”Ÿæˆçš„æ–‡æœ¬æ›´åŠ å®‰å…¨å¯é 
  - å¹³è¡¡æ¨¡å¼
    - é€šå¸¸æ˜¯0.8ï¼Œç”Ÿæˆçš„æ–‡æœ¬é€šå¸¸æœ‰ä¸€å®šçš„å¤šæ ·æ€§ï¼Œåˆèƒ½ä¿æŒè¾ƒå¥½çš„è¿è´¯æ€§å’Œå‡†ç¡®æ€§
  - åˆ›æ„æ¨¡å¼
    - é€šå¸¸æ˜¯1ï¼Œç”Ÿæˆçš„æ–‡æœ¬æ›´æœ‰åˆ›æ„ï¼Œä½†ä¹Ÿæ›´å®¹æ˜“å‡ºç°è¯­æ³•é”™è¯¯æˆ–ä¸åˆé€»è¾‘çš„å†…å®¹
- max_tokens
  - é™åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼Œé˜²æ­¢è¾“å‡ºè¿‡é•¿
  - è®¾ç½®å»ºè®®
    - çŸ­å›å¤ï¼š128-256
    - å¸¸è§„å¯¹è¯ã€å¤šè½®å¯¹è¯ï¼š512-1024
    - é•¿å†…å®¹ç”Ÿæˆï¼š1024-4096
  - token
    - å¤§æ¨¡å‹å¤„ç†æ–‡æœ¬çš„æœ€å°å•ä½ï¼Œç›¸å½“äºè‡ªç„¶è¯­è¨€ä¸­çš„è¯æˆ–å­—ï¼Œè¾“å‡ºæ—¶é€ä¸ªtokenä¾æ¬¡ç”Ÿæˆ
    - ä¸€èˆ¬æ”¶è´¹æ ‡å‡†1ä¸ªtokençº¦ç­‰äº1-1.8ä¸ªæ±‰å­—æˆ–3-4ä¸ªè‹±æ–‡å­—æ¯
    - [tokenå­—ç¬¦è½¬åŒ–å·¥å…·](https://platform.openai.com/tokenizer)

## å¯¹è¯æ¨¡å‹çš„message

- SystemMessage
  - è®¾å®šAIè¡Œä¸ºè§„åˆ™æˆ–èƒŒæ™¯ä¿¡æ¯ã€‚æ¯”å¦‚è®¾å®šAIçš„åˆå§‹çŠ¶æ€ã€è¡Œä¸ºæ¨¡å¼æˆ–å¯¹è¯çš„æ€»ä½“ç›®æ ‡ã€‚æ¯”å¦‚ï¼šâ€œä½œä¸ºä¸€ä¸ªä»£ç ä¸“å®¶â€ï¼Œæˆ–è€…â€œè¿”å›JSONæ ¼å¼â€ã€‚é€šå¸¸æ˜¯è¾“å…¥æ¶ˆæ¯åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªä¼ é€’
- HumanMessage
  - è¡¨ç¤ºæ¥è‡ªç”¨æˆ·è¾“å…¥
- AIMessage
  - å­˜å‚¨AIå›å¤çš„å†…å®¹ï¼Œå¯ä»¥æ˜¯æ–‡æœ¬ï¼Œä¹Ÿå¯ä»¥æ˜¯è°ƒç”¨å·¥å…·çš„è¯·æ±‚
- ChatMessage
  - å¯ä»¥è‡ªå®šä¹‰è§’è‰²çš„é€šç”¨æ¶ˆæ¯ç±»å‹
- FunctionMessage/ToolMessage
  - å‡½æ•°è°ƒç”¨/å·¥å…·æ¶ˆæ¯ï¼Œç”¨äºå‡½æ•°è°ƒç”¨ç»“æœçš„æ¶ˆæ¯ç±»å‹

## æ¨¡å‹è°ƒç”¨çš„æ–¹æ³•

- invoke
  - å¤„ç†å•æ¡è¾“å…¥ï¼Œç­‰å¾…LLMå®Œå…¨æ¨ç†å®Œæˆåå†è¿”å›è°ƒç”¨ç»“æœ
- stream
  - æµå¼å“åº”ï¼Œé€å­—è¾“å‡ºLLMçš„å“åº”ç»“æœ
- batch
  - å¤„ç†æ‰¹é‡è¾“å…¥
æœ‰ç›¸åº”çš„å¼‚æ­¥æ–¹æ³•ï¼Œä¸asyncioå’Œawaitè¯­æ³•ä¸€èµ·ä½¿ç”¨å®ç°å¹¶å‘
- astream
  - å¼‚æ­¥æµå¼å“åº”
- ainvoke
  - å¼‚æ­¥å¤„ç†å•æ¡è¾“å…¥
- abatch
  - å¼‚æ­¥å¤„ç†æ‰¹é‡è¾“å…¥
- astream_log
  - å¼‚æ­¥æµå¼è¿”å›ä¸­é—´æ­¥éª¤ï¼Œä»¥åŠæœ€ç»ˆç»“æœ

## æç¤ºè¯æ¨¡æ¿

- PromptTemplate
  - ç”¨äºç”Ÿæˆå­—ç¬¦ä¸²æç¤º
- ChatPromptTemplate
  - èŠå¤©æç¤ºæ¨¡æ¿ï¼Œç”¨äºç»„åˆå„ç§è§’è‰²çš„æ¶ˆæ¯æ¨¡æ¿ï¼Œä¼ å…¥èŠå¤©æ¨¡å‹
- XxxMessagePromptTemplate
  - æ¶ˆæ¯æ¨¡æ¿è¯æ¨¡æ¿ï¼ŒåŒ…æ‹¬ï¼šSystemMessagePromptTemplateã€HumanMessagePromptTemplateã€AIMessagePromptTemplateã€ChatMessagePromptTemplateç­‰
- FewShotPromptTemplate
  - æ ·æœ¬æç¤ºè¯æ¨¡æ¿ï¼Œé€šè¿‡ç¤ºä¾‹æ¥æ•™æ¨¡å‹å¦‚ä½•å›ç­”
- PipelinePrompt
  - ç®¡é“æç¤ºè¯æ¨¡æ¿ï¼Œå°†å¤šä¸ªæç¤ºè¯æ¨¡æ¿ç»„åˆèµ·æ¥ä¸€èµ·ä½¿ç”¨
- è‡ªå®šä¹‰æ¨¡æ¿
  - å…è®¸åŸºäºå…¶ä»–æ¨¡æ¿ç±»æ¥å®šåˆ¶è‡ªå·±çš„æç¤ºè¯æ¨¡æ¿

```python
from Langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    template="è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}",
)
prompt_template.format(question="ä½ æ˜¯è°") # è¿”å›å€¼ç±»å‹ä¸ºå­—ç¬¦ä¸²
prompt_template.invoke(input={"question": "ä½ æ˜¯è°"}) # è¿”å›å€¼ç±»å‹ä¸ºPromptValue

prompt_template = PromptTemplate.from_template(
    template="è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}, å¹¶ä¿æŒå›ç­”çš„ä¸“ä¸šå’Œå‡†ç¡®æ€§, è¿”å›æ ¼å¼{format}",
    partial_variables={"question": "AIèƒ½å¤Ÿåšä»€ä¹ˆ",},
)
prompt_template.format(format="JSON")
```

### ChatPromptTemplate

```python
from Langchain_core.prompts import ChatPromptTemplate

chat_prompt_template = ChatPromptTemplate.from_messages(
    messages=[
        ("system", "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œåå­—å«{name}"),
        ("human", "{question}"),
    ],
    #input_variables=["role", "name", "question"],
) 
## è¿”å›ç±»å‹ä¸ºChatPromptValue
chat_prompt_template.invoke(input={"role": "ä¸“ä¸šçš„ä»£ç åŠ©æ‰‹", "name": "AIåŠ©æ‰‹", "question": "ä½ æ˜¯è°"})  
## è¿”å›ç±»å‹ä¸ºList
chat_prompt_template.format_messages(role="ä¸“ä¸šçš„ä»£ç åŠ©æ‰‹", name="AIåŠ©æ‰‹", question="ä½ æ˜¯è°")

## å®ç°ChatPromptValueã€Listã€å­—ç¬¦ä¸²ä¹‹é—´çš„è½¬æ¢
chat_prompt_template.to_string()
chat_prompt_template.to_messages()
```

## æ’å…¥æ¶ˆæ¯åˆ—è¡¨

```python
# å½“ChatPromptTemplateæ¨¡æ¿ä¸­çš„æ¶ˆæ¯ç±»å‹å’Œä¸ªæ•°ä¸ç¡®å®šçš„æ—¶å€™ï¼Œå¯ä»¥ä½¿ç”¨MessagePlaceholder
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompts.chat import MessagePlaceholder
from langchain_core.messages import HumanMessage, AIMessage

chat_prompt_template = ChatPromptTemplate.from_messages(
    messages=[
        ("system", "ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œåå­—å«{name}"),
        MessagePlaceholder(variable_name="history"),
        ("human", "{question}"),
    ],
)

chat_prompt_template.invoke({
  "role": "ä¸“ä¸šçš„ä»£ç åŠ©æ‰‹", 
  "name": "AIåŠ©æ‰‹", 
  # "messages": [("human", "ä½ æ˜¯è°")],
  "history": [HumanMessage(content="ä½ æ˜¯è°"), AIMessage(content="æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åŠ©æ‰‹")],
  "question": "æˆ‘åˆšæ‰çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"
})
```

## å°‘é‡ç¤ºä¾‹æç¤ºè¯æ¨¡æ¿

FewShotPromptTemplateä¸PromptTemplateä¸€èµ·ä½¿ç”¨
FewShotChatMessagePromptTemplate ä¸ChatPromptTemplateä¸€èµ·ä½¿ç”¨

```python
example_prompt_template = FewShotPromptTemplate(
    example_prompt=PromptTemplate.from_template(
        template="é—®é¢˜ï¼š{question}\nå›ç­”ï¼š{answer}",
    ),
    examples=[
        {"question": "ä½ æ˜¯è°", "answer": "æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åŠ©æ‰‹"},
        {"question": "ä½ èƒ½åšä»€ä¹ˆ", "answer": "æˆ‘å¯ä»¥å›ç­”ä»£ç ç›¸å…³çš„é—®é¢˜"},
    ],
    suffix="é—®é¢˜ï¼š{question}\nå›ç­”ï¼š",
    input_variables=["question"],
)
```

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate
example_prompt_template = FewShotChatMessagePromptTemplate(
    example_prompt=ChatPromptTemplate.from_messages(
        messages=[
            ("human", "{question} æ˜¯å¤šå°‘ï¼Ÿ"),
            ("ai", "{answer}"),
        ],
    ),
    examples=[
        {"question": "2 ğŸŒˆ 2", "answer": "4"},
        {"question": "2 ğŸŒˆ 3", "answer": "8"},
    ],
)
final_prompt_template = ChatPromptTemplate.from_messages(
    messages=[
        ("system", "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“ä¸šçš„åŠ©æ‰‹"),
        example_prompt_template,
        ("human", "{question}"),
    ],
)
```
